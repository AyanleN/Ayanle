{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 Author: Emma LeRoy Advisor: Tyson Lee Swetnam About \u00b6 Emma LeRoy is a rising senior at University High School in Tucson, Arizona. During the summer of 2022 she is an intern at The University of Arizona through the BIO5 Institute's KEYS program. This summer she is an intern working at The University of Arizona in Dr. Tyson L. Swetnam's lab: a member of CyVerse , a cutting edge cyberinfrastructure funded by the National Science Foundation that is designed for research and committed to the principles of open science. This website follows the FAIR and CARE data principles and hopes to help further open science.","title":"Introduction"},{"location":"#introduction","text":"Author: Emma LeRoy Advisor: Tyson Lee Swetnam","title":"Introduction"},{"location":"#about","text":"Emma LeRoy is a rising senior at University High School in Tucson, Arizona. During the summer of 2022 she is an intern at The University of Arizona through the BIO5 Institute's KEYS program. This summer she is an intern working at The University of Arizona in Dr. Tyson L. Swetnam's lab: a member of CyVerse , a cutting edge cyberinfrastructure funded by the National Science Foundation that is designed for research and committed to the principles of open science. This website follows the FAIR and CARE data principles and hopes to help further open science.","title":"About"},{"location":"STACcatalogs/","text":"STAC Catalogs \u00b6 What Are They? \u00b6 Oftentimes raw data is not ready for analysis and requires pre-processing and the installation of a significant number of software packages (Fergason et al., 2021) allows for creation of metadata to make datasets indexable and discoverable (The STAC Specification, n.d.) promotes reproducibility, open data, and open science STAC specification means that these large datasets can be used on the cloud as opposed to being downloaded locally, shortens run time and improves ease of use Figure credit : STAC Specification official logo from the STAC Index Site Structure \u00b6 composed of spatio-temporal assets- a piece of planetary data collected at a specific time with a specific location (The STAC Specification, n.d.) STAC specification designates inclusion of metadata on time and location of data, thumbnail for searching and discovery, applicable links to raw data, key words to describe relationship of data and point to similar spatio-temporal assets (The STAC Specification, n.d.) Users can further customize this metadata Spatio-temporal assets are a type of Javascript Object Notation (JSON) File JSON files are a data interchange format that allow datasets to be displayed in a text based format (The STAC Specification, n.d.) How Are They Used? \u00b6 used to index data, making it searchable and discoverable data standard for formatting and creating metadata for geospatial data promotes open data and reproducibility allows data to be shared, understood, and discovered used for geospatial data about the Earth; however can also be used for planetary data (Fergason et al., 2021) Can be viewed as a webpage using a STAC Browser Use in Google's Earth Engine \u00b6 The GEE Public Data Catalog can be viewed as a STAC catalog , STAC browser , or HTML version allows searchability, discovery, and improves user functionality","title":"STAC Catalogs"},{"location":"STACcatalogs/#stac-catalogs","text":"","title":"STAC Catalogs"},{"location":"STACcatalogs/#what-are-they","text":"Oftentimes raw data is not ready for analysis and requires pre-processing and the installation of a significant number of software packages (Fergason et al., 2021) allows for creation of metadata to make datasets indexable and discoverable (The STAC Specification, n.d.) promotes reproducibility, open data, and open science STAC specification means that these large datasets can be used on the cloud as opposed to being downloaded locally, shortens run time and improves ease of use Figure credit : STAC Specification official logo from the STAC Index Site","title":"What Are They?"},{"location":"STACcatalogs/#structure","text":"composed of spatio-temporal assets- a piece of planetary data collected at a specific time with a specific location (The STAC Specification, n.d.) STAC specification designates inclusion of metadata on time and location of data, thumbnail for searching and discovery, applicable links to raw data, key words to describe relationship of data and point to similar spatio-temporal assets (The STAC Specification, n.d.) Users can further customize this metadata Spatio-temporal assets are a type of Javascript Object Notation (JSON) File JSON files are a data interchange format that allow datasets to be displayed in a text based format (The STAC Specification, n.d.)","title":"Structure"},{"location":"STACcatalogs/#how-are-they-used","text":"used to index data, making it searchable and discoverable data standard for formatting and creating metadata for geospatial data promotes open data and reproducibility allows data to be shared, understood, and discovered used for geospatial data about the Earth; however can also be used for planetary data (Fergason et al., 2021) Can be viewed as a webpage using a STAC Browser","title":"How Are They Used?"},{"location":"STACcatalogs/#use-in-googles-earth-engine","text":"The GEE Public Data Catalog can be viewed as a STAC catalog , STAC browser , or HTML version allows searchability, discovery, and improves user functionality","title":"Use in Google's Earth Engine"},{"location":"communityDataset/","text":"Awesome-gee-community-datasets \u00b6 Flowchart for the STAC specification used with the community datasets \u00b6 Flowchart for the project markdown files that are rendering on the github.io site \u00b6 However, this information varies slightly for each dataset: - some are missing key info such as the DOI - others include different additional specific information on the dataset that are an exception from the norm - additionally, the descriptions vary heavily in detail between the datasets It appears that the user is contributing this information for the project file when they submit their dataset to be included in the project. There are procedures for submitting datasets, including what info to include; however, it seems that this isn't being strictly adhered to. Question: are the contributors also providing the information for the JSON files? because these seem much more correctly standardized to the STAC specification guidelines Notes on Earth Engine Visualizations \u00b6 Community Dataset Shortlist","title":"Awesome Community Datasets"},{"location":"communityDataset/#awesome-gee-community-datasets","text":"","title":"Awesome-gee-community-datasets"},{"location":"communityDataset/#flowchart-for-the-stac-specification-used-with-the-community-datasets","text":"","title":"Flowchart for the STAC specification used with the community datasets"},{"location":"communityDataset/#flowchart-for-the-project-markdown-files-that-are-rendering-on-the-githubio-site","text":"However, this information varies slightly for each dataset: - some are missing key info such as the DOI - others include different additional specific information on the dataset that are an exception from the norm - additionally, the descriptions vary heavily in detail between the datasets It appears that the user is contributing this information for the project file when they submit their dataset to be included in the project. There are procedures for submitting datasets, including what info to include; however, it seems that this isn't being strictly adhered to. Question: are the contributors also providing the information for the JSON files? because these seem much more correctly standardized to the STAC specification guidelines","title":"Flowchart for the project markdown files that are rendering on the github.io site"},{"location":"communityDataset/#notes-on-earth-engine-visualizations","text":"Community Dataset Shortlist","title":"Notes on Earth Engine Visualizations"},{"location":"cyverse/","text":"CyVerse \u00b6 Introduction \u00b6 CyVerse serves as a way for life scientists to share their data with others around the world through a common cyberinfrastructure. It is funded by the National Science Foudation's Directorate for Biological Science and is currently led by the University of Arizona. CyVerse's cyberinfrastructure allows scientists to store their data as well as share it with others through cloud computing for further analysis. Acting as a way to complete complex analyses and share large datasets, CyVerse furthers open science and enables a collaborative workspace. VICE Apps \u00b6 The Visual and Interactive Computing Environment(VICE) allows scientists to run interactive applications through CyVerse. Through this scientists can open their interactive applications(Jupyter Lab, RStudio, Shiny, WebGL, HTML5, VNC, and XPRA), transfer data into containters, analyze this data, and send their results to the cloud.","title":"CyVerse"},{"location":"cyverse/#cyverse","text":"","title":"CyVerse"},{"location":"cyverse/#introduction","text":"CyVerse serves as a way for life scientists to share their data with others around the world through a common cyberinfrastructure. It is funded by the National Science Foudation's Directorate for Biological Science and is currently led by the University of Arizona. CyVerse's cyberinfrastructure allows scientists to store their data as well as share it with others through cloud computing for further analysis. Acting as a way to complete complex analyses and share large datasets, CyVerse furthers open science and enables a collaborative workspace.","title":"Introduction"},{"location":"cyverse/#vice-apps","text":"The Visual and Interactive Computing Environment(VICE) allows scientists to run interactive applications through CyVerse. Through this scientists can open their interactive applications(Jupyter Lab, RStudio, Shiny, WebGL, HTML5, VNC, and XPRA), transfer data into containters, analyze this data, and send their results to the cloud.","title":"VICE Apps"},{"location":"githubactions/","text":"Introduction \u00b6 GitHub Actions allows for the automation of tasks within your software development life cycle. Through GitHub actions users can automatically run their software testing scripts. Key Vocabulary \u00b6 Workflows \u00b6 Workflows can be used to design, test, package, release, or deploy a project on GitHub. A workflow can be added to a repository in GitHub using the file name .github/workflows. Workflows consist of one or more jobs that are scheduled/triggered by an event. Events \u00b6 Events are the activities that trigger the start of a workflow. Workflows can be triggered using pus or pull requests. Runners \u00b6 Runners can be hosted by GitHub or own on your own and is basically considered as a server that has GitHub Actions runner application installed. A runner runs one job at a time and reports the progress to GitHub. Jobs \u00b6 A job is a sequence of instructions that run on the same runner. A workflow containing multiple jobs will perform them in parallel by default. You may also set up a pipeline to conduct jobs in a specific order. Steps \u00b6 A step is a single task that can be used to execute commands in a job. An action or a shell command can be used as a step. Each step of a task runs on the same runner, allowing all of the activities in that job to exchange data. Actions \u00b6 Actions are commands that are used to join steps to create a job. Actions can be created or found on the GitHub community. Figure credit : GitHub Docs The components of GitHub Actions that work together to run jobs","title":"Githubactions"},{"location":"githubactions/#introduction","text":"GitHub Actions allows for the automation of tasks within your software development life cycle. Through GitHub actions users can automatically run their software testing scripts.","title":"Introduction"},{"location":"githubactions/#key-vocabulary","text":"","title":"Key Vocabulary"},{"location":"githubactions/#workflows","text":"Workflows can be used to design, test, package, release, or deploy a project on GitHub. A workflow can be added to a repository in GitHub using the file name .github/workflows. Workflows consist of one or more jobs that are scheduled/triggered by an event.","title":"Workflows"},{"location":"githubactions/#events","text":"Events are the activities that trigger the start of a workflow. Workflows can be triggered using pus or pull requests.","title":"Events"},{"location":"githubactions/#runners","text":"Runners can be hosted by GitHub or own on your own and is basically considered as a server that has GitHub Actions runner application installed. A runner runs one job at a time and reports the progress to GitHub.","title":"Runners"},{"location":"githubactions/#jobs","text":"A job is a sequence of instructions that run on the same runner. A workflow containing multiple jobs will perform them in parallel by default. You may also set up a pipeline to conduct jobs in a specific order.","title":"Jobs"},{"location":"githubactions/#steps","text":"A step is a single task that can be used to execute commands in a job. An action or a shell command can be used as a step. Each step of a task runs on the same runner, allowing all of the activities in that job to exchange data.","title":"Steps"},{"location":"githubactions/#actions","text":"Actions are commands that are used to join steps to create a job. Actions can be created or found on the GitHub community. Figure credit : GitHub Docs The components of GitHub Actions that work together to run jobs","title":"Actions"},{"location":"githubed/","text":"GitHub Education \u00b6 Steps to Enroll \u00b6 Go to the GitHub Education Site and enter your education status as student From here your school email and dated documentation of your enrollment is required After this is approved you have access to the GitHub education student developer pack! What's Included and Functionality \u00b6 As part of the GitHub education student developer pack and GitHub global campus students and faculty are granted access to forums, Campus TV, exclusive events, and free software and subscriptions such as Canva Pro, Microsoft Azure, and VS code. note: the student developer pack doesn't include access to GitHub CodeSpaces","title":"GitHub Education Access"},{"location":"githubed/#github-education","text":"","title":"GitHub Education"},{"location":"githubed/#steps-to-enroll","text":"Go to the GitHub Education Site and enter your education status as student From here your school email and dated documentation of your enrollment is required After this is approved you have access to the GitHub education student developer pack!","title":"Steps to Enroll"},{"location":"githubed/#whats-included-and-functionality","text":"As part of the GitHub education student developer pack and GitHub global campus students and faculty are granted access to forums, Campus TV, exclusive events, and free software and subscriptions such as Canva Pro, Microsoft Azure, and VS code. note: the student developer pack doesn't include access to GitHub CodeSpaces","title":"What's Included and Functionality"},{"location":"googleEarthEngine/","text":"Google's Earth Engine \u00b6 Use \u00b6 Established in 2010 as a geospatial analysis platform (Kumar & Mutanga, 2018) run in the cloud (allows analysis of large datasets) public datasets promotes open science and reproducibility cloud computing allows planetary scale analysis (Kumar & Mutanga, 2018) users can upload their own datasets and write their own scripts to perform analyses (Kumar & Mutanga, 2018) The development of GEE as a free resource creates equity within science brings a high level analysis tool to a wider range of people, which furthers open science and promotes inquiry, discovery, and collaboration Figure credit : Image created by Diana Krupnik for article on teaching using the GEE API APIs \u00b6 Javascript Python Rest Structure \u00b6 initially featured a data repository from the past 40 years of global remote sensing data now expanded to include vector, climate, demographic, and elevation data These datasets can be layered to perform complex geospatial analyses community datasets are open source Code Editor \u00b6 Web based IDE for writing scripts (Get Started with Earth Engine | Google Earth Engine |, n.d.) complex geospatial work flows made easy interactive environment for developing earth engine applications raster data type is a matrix of cells in grid format -- digital aerial photographs, scanned maps","title":"Google Earth Engine"},{"location":"googleEarthEngine/#googles-earth-engine","text":"","title":"Google's Earth Engine"},{"location":"googleEarthEngine/#use","text":"Established in 2010 as a geospatial analysis platform (Kumar & Mutanga, 2018) run in the cloud (allows analysis of large datasets) public datasets promotes open science and reproducibility cloud computing allows planetary scale analysis (Kumar & Mutanga, 2018) users can upload their own datasets and write their own scripts to perform analyses (Kumar & Mutanga, 2018) The development of GEE as a free resource creates equity within science brings a high level analysis tool to a wider range of people, which furthers open science and promotes inquiry, discovery, and collaboration Figure credit : Image created by Diana Krupnik for article on teaching using the GEE API","title":"Use"},{"location":"googleEarthEngine/#apis","text":"Javascript Python Rest","title":"APIs"},{"location":"googleEarthEngine/#structure","text":"initially featured a data repository from the past 40 years of global remote sensing data now expanded to include vector, climate, demographic, and elevation data These datasets can be layered to perform complex geospatial analyses community datasets are open source","title":"Structure"},{"location":"googleEarthEngine/#code-editor","text":"Web based IDE for writing scripts (Get Started with Earth Engine | Google Earth Engine |, n.d.) complex geospatial work flows made easy interactive environment for developing earth engine applications raster data type is a matrix of cells in grid format -- digital aerial photographs, scanned maps","title":"Code Editor"},{"location":"jupyter/","text":"Jupyter Notebooks \u00b6 What is it? \u00b6 seamless way to write and iterate python code to perform data analysis Notebooks allows code, figures, diagrams, charts, and explanations to all be stored in one location This allows developer's work to be shared, leading to collaboration and improving reproducibility Additionally, Jupyter Notebooks is free of charge, improving equity in data analysis and software development When new notebooks are created prebuilt docker containers are used to put the notebooks at their own path How does it work? \u00b6 The basis for Jupyter Notebooks is IPython, a command line shell for writing code in the terminal Juputer Notebooks allows this code to be written and iterated in the browser through the use of the ipykernel lines of code can be run all at once or one at a time Jupyter Notebooks supports mulitple languages, most common is python Allows storage of code and inclusion of Markdown files for notes and documentation when new notebooks are created prebuilt docker containers are used to place the notebooks on their own paths Structure \u00b6 Kernel \u00b6 the kernel is specific to the programming language, this project will be in python process that supports the notebook to execute the written code Juptyer team maintains the ipykernel, but other user maintained kernels are available for use Cell \u00b6 the cells are the main contents of the notebooks and are where code is written markdown cells can be created to store notes and info on code green = code running grey = code not running","title":"Jupyter Notebooks"},{"location":"jupyter/#jupyter-notebooks","text":"","title":"Jupyter Notebooks"},{"location":"jupyter/#what-is-it","text":"seamless way to write and iterate python code to perform data analysis Notebooks allows code, figures, diagrams, charts, and explanations to all be stored in one location This allows developer's work to be shared, leading to collaboration and improving reproducibility Additionally, Jupyter Notebooks is free of charge, improving equity in data analysis and software development When new notebooks are created prebuilt docker containers are used to put the notebooks at their own path","title":"What is it?"},{"location":"jupyter/#how-does-it-work","text":"The basis for Jupyter Notebooks is IPython, a command line shell for writing code in the terminal Juputer Notebooks allows this code to be written and iterated in the browser through the use of the ipykernel lines of code can be run all at once or one at a time Jupyter Notebooks supports mulitple languages, most common is python Allows storage of code and inclusion of Markdown files for notes and documentation when new notebooks are created prebuilt docker containers are used to place the notebooks on their own paths","title":"How does it work?"},{"location":"jupyter/#structure","text":"","title":"Structure"},{"location":"jupyter/#kernel","text":"the kernel is specific to the programming language, this project will be in python process that supports the notebook to execute the written code Juptyer team maintains the ipykernel, but other user maintained kernels are available for use","title":"Kernel"},{"location":"jupyter/#cell","text":"the cells are the main contents of the notebooks and are where code is written markdown cells can be created to store notes and info on code green = code running grey = code not running","title":"Cell"},{"location":"keysassignments/","text":"Keys Assignments \u00b6 Assignment 1: Internship Description \u00b6 This summer I am working in Dr. Tyson L. Swetnam's Lab. He is a Research Assistant Professor of Geoinformatics with a joint appointment in the School of Natural Resources and the Environment. Dr. Swetnam is part of the CyVerse initiative: a National Science Foundation funded cyberinfrastructure that promotes open source data, science, and collaboration in the biosciences. Dr. Swetnam's research is focused on using cyberinfrastructure to support and encourage reproducible research, along with geospatial analysis. The Lab's current projects include Open Dendro , an initiative to restructure outdated dendrochronological software written in C, R, and Fortran into Python. Dr. Swetnam is involved in a plethora of other projects, but all of his projects show a commitment to open science through the use of open data and open software to promote reproducible research in information science. The primary project that I will be focused on this summer is the improvement of spatio-temporal asset catalog (STAC) catalogs for use in Google's Earth Engine. Each spatio-temporal asset is a file or dataset that contains information about the Earth captured at a specific time. By streamlining this process and working on making these datasets more accessible it ensures equal access and promotes open science. Project Description \u00b6 This summer I will be working on improving the STAC catalogs for the awesome-gee-community-datasets. This is a project spearheaded by Dr. Swetnam's former graduate student and colleague, Samapriya Roy. The project is focused on improving datasets for community use with Google's Earth Engine Catalog, a tool for mapping satellite and geospatial data across the Earth's surface to detect changes and trends. The awesome-gee-community-datasets are a set of community gathered and organized geospatial data that is preprocessed to allow for easy use with Google's Earth Engine. This summer I will work on improving the spatial-temporal asset (STAC) catalogs for these datasets to improve ease of use and access. A STAC catalog serves as a standardized way of storing and indexing geospatial data for easy discovery and use. This creates a common language and format that ensures previous code doesn't need to be rewritten, promoting open data, software, and science. Assignment 2: Introduction to your Research \u00b6 Purpose: \u00b6 The purpose of my research is to standardize and process geospatial data for use with Google\u2019s Earth Engine for planetary-scale geospatial analysis. This improvement and standardization of geospatial data will be conducted using spatio-temporal asset catalogs (STAC), which will help make these complex datasets easier to index and discover in order to perform analyses of Earth\u2019s systems and conditions. Previous Research: \u00b6 Oftentimes raw data is not ready for analysis, and it requires pre-processing. Additionally, this analysis often requires the installation and utilization of many different software packages. This is especially true for large datasets that are being used for planetary analysis on an Earth and solar system scale. However, the use of the spatio-temporal asset catalog (STAC) specification allows for the creation of metadata that allows datasets to be indexable and discoverable. A spatio-temporal asset is a type of Javascript object notation (JSON) file. This is a data interchange format that allows large raw datasets to be text based. The STAC specification designates the need for metadata on the time and location of the data, a thumbnail, links to the raw data, and key words that describe the relationship of the data, pointing to similar spatio-temporal assets. Following this specification furthers cloud based computing and means that these large geospatial datasets don\u2019t need to be downloaded locally for planetary scale analysis. It is for these reasons that previous research has shown that the STAC specification is an important and highly useful method for designating metadata, especially for large planetary scale analysis (Fergason et al, 2018). Additionally, this STAC specification can be used for Google\u2019s Earth Engine (GEE), established in 2010. GEE initially featured a data repository of global satellite data from the past 40 years; however, this has expanded to now include vector, climate, demographic, and elevation data that can be layered to perform complex global geospatial analyses. Users can also upload their own datasets and write scripts to analyze this data. The development of GEE as a free resource has helped to level the playing field and make large-scale geospatial analysis possible for a larger range of scientists, particularly those in developing countries (Kumar and Mutanga, 2018). However, a lot of this data that users are looking to use with GEE is not preproccessed, and STAC catalogs serve as a potential solution to this. My project will allow more equitable acces to a variety of geospatial data, promoting equality and open science. By improving the use of STAC catalogs it ensures that scientists have access to quality analysis tools like GEE and datasets that are easy to navigate. Need For Study: \u00b6 This study will further the use of STAC catalogs for geospatial data. Through this standardization, it will make Google\u2019s Earth Engine more accessible to other researchers and data scientists. Pre-processing geospatial data and formatting it using STAC catalogs makes this data easier to index and search, allowing for its discovery. This concept of open data is crucial in regard to the open science movement. The production of open software and data allows key geospatial analysis tools to reach a larger number of researchers and scientists. With the threat of climate change and the necessity for immediate action, it is crucial to understand Earth\u2019s systems and processes. Google\u2019s Earth Engine allows for large-scale geospatial analysis that can help scientists visualize and understand concepts such as forest fire management and sea level change. The improvement of STAC catalogs for Google\u2019s Earth Engine facilitates analysis and discovery that is crucial to understanding how to prevent climate change and preserve the natural environment. Problem Statement: \u00b6 Many current geospatial datasets are difficult to navigate, discover, and analyze. How can the organization of this data be improved to ensure ease of use, appropriate analysis, and equitable access? References \u00b6 Fergason, R. L., Hunter, M. A., Laura, J. R., Hare, T. M., & U.S. Geological Survey. (2021). Analysis Ready Data Available Through the SpatioTemporal Asset Catalog (STAC) Specification: Investigating the Application to Planetary Data. 5 th Planetary Data and PSIDA 2021, 2549, 7023\u20137024. https://www.hou.usra.edu/meetings/planetdata2021/pdf/7023.pdf Kumar, L., & Mutanga, O. (2018). Google Earth Engine Applications Since Inception: Usage, Trends, and Potential. Remote Sensing, 10(10), 1509. https://doi.org/10.3390/rs10101509 Assignment 3: Materials and Methods \u00b6 To best standardize and organize geospatial data for use with Google\u2019s Earth Engine I will use the Spatio-Temporal Asset Catalog (STAC) Specification. The STAC Specification is the leading standard for geospatial data because it allows for the creation of metadata (data that provides information on the datasets) that allows assets to be indexable and discoverable. By following this specification I will create metadata for the awesome-gee-community datasets to improve reproducibility and promote discovery. Throughout this process I will use free software such as GitHub, VSCode, and Jupyter Notebooks to improve the metadata to align with the STAC Specification: - GitHub is a platform for software development that promotes collaboration and is committed to open science - VSCode is a code editor that provides syntax and debugging support - Jupyter Notebooks is a web-based computing platform that allows for the seamless integration of a code editor, terminal, and markdown files for documentation This software will aid me in completing my project efficiently and effectively. Additionally, their free nature and public access aligns with my project\u2019s mission of furthering open and equitable science. The initial organization of the awesome-gee-community datasets on GitHub is represented by this diagram: Using the aforementioned software, I will use the coding languages of Python, Markdown, and Javascript to create and improve the awesome-gee-community datasets\u2019 metadata to align with the STAC Specification, allowing its use with Google\u2019s Earth Engine. Assignment 4: Results \u00b6 I\u2019m still in the middle of conducting my research; therefore, I don\u2019t have completed results yet. Additionally, my project is somewhat unconventional in nature, as I am processing and standardizing datasets. This means that I won\u2019t have much numerical, collected data to graph or represent in charts. Instead, I plan to include information on which datasets I processed, and what the result of processing them was. I\u2019m currently working with around 50 datasets; however, I\u2019m not sure if I will work through all of these datasets during the internship and there is no logistical way to represent all of them on one poster. However, all of the datasets will be processed and standardized with the same template, producing a similar result but with different data. Therefore, my plan is to document the results of implementing this process for one example dataset, and then also include information on all of the other datasets that I processed. I haven\u2019t begun this process yet because my PI is still working on finalizing the guidelines and template for the process; therefore, I\u2019m not exactly sure what this will look like. I plan to represent my results mostly through visuals. I will include a screenshot of the final, processed spatio-temporal asset, along with a screenshot of this processed data visualized with Google Earth Engine that will look something like this: Ivushkin, Konstantin, Harm Bartholomeus, Arnold K. Bregt, Alim Pulatov, Bas Kempen, and Luis De Sousa. \"Global mapping of soil salinity change.\" Remote sensing of environment 231 (2019): 111260. I will then explain that all of the geospatial datasets were processed in this similar manner to achieve usable, standardized results. Additionally, I will include information on all of the geospatial data that I processed, along with what category it falls into, and its citation. I will represent this through a concept map/flowchart that will look something like this: Fig 1. Visual representation for listing the names of the standardized data sets and their corresponding overall categories My project is all done through computing, and therefore there is a large volume of code that can\u2019t be realistically all shown on my poster. However, throughout my internship I have been documenting my progress and results on a website I built. I need to further discuss this with my PI, but I also may include a QR code to my website under my results section for viewers who are interested in exploring my project more fully. Website QR code for extra information on my project and process Assignment 5 Long Abstract \u00b6 Many geospatial datasets are difficult to access, require preprocessing, or the use of specialized software. This creates a problem for data scientists who are researching the Earth and performing planetary scale analyses: many of these datasets are hard to discover and navigate. In order to combat this issue, data standardization is necessary. The Spatio-Temporal Asset Catalog (STAC) specification provides a solution to this by designating the creation of metadata for each set of geospatial data, improving indexability and discovery. This data can be visualized and analyzed through Google\u2019s Earth Engine (GEE), a cloud based computing platform that allows for powerful, planetary scale geospatial analysis. We employed this standardization technique for Dr. Samapriya Roy\u2019s Awesome-Gee-Community-Datasets, a GitHub repository of over 850 geospatial datasets that can be analyzed and understood through GEE. By narrowing in on a subset of 49 of these datasets, Dr. Roy created a template that designated the creation and standardization of metadata for each of these datasets. This designated the need for information including a description, source data structure, GEE code snippet, license, and citation. This final, standardized metadata was displayed on the community datasets GitHub website, providing data scientists further knowledge on how to use, cite, and discover this data for analysis with GEE. Standardizing this data promotes discovery and the principles of open science, ensuring equity within computing through open data and open code. This is crucial as scientists work collaboratively to understand the planet\u2019s systems and processes to better combat climate change. Assignment 6 Conclusion and Discussion \u00b6 Due to the complex nature of large geospatial datasets, they can be difficult to analyze, requiring large amounts of preprocessing or software, stunting collaboration and scientific progress. This process for the standardization of the Community Datasets\u2019 metadata was successful in creating a more uniform format. By editing the MarkDown files to ensure the documentation was uniform throughout, it improved the user interface for the Community Datasets repository. We were not able to work through all of the forty-nine dataset subset, which was largely due to the long period of background research. In order to understand the current organizational status of the repository, and thus assess how to best standardize it, a significant period of time was spent taking notes and data on the initial state of the repository. While time consuming, this was a crucial step to ensure a replicable and effective technique for standardization, even if it meant sacrificing the quantity of standardized datasets. Additionally, some template elements, particularly the source data structure, were difficult, or not possible at this time, to ascertain. This meant that some of the standardized datasets did not perfectly match the initial template. Although some of the datasets did not align perfectly, the integrity of the project was still preserved, as usability and organization was still improved. This project will be continued to extend to the other datasets in the repository, hopefully eventually working through all remaining datasets, and ensuring that new datasets meet the specification when added to the repository. By improving the access and structure of these datasets, their discoverability is also improved. This upholds the tenets of open science through the principles of open data and open code. This plays an important role in improving equity and collaboration in computing. By providing data scientists increased and improved tools for analysis and discovery, it helps further understanding of Earth\u2019s systems and processes, helping us build a more unbiased, conscientious, and aware society. Assignment 7 Title and Short Abstract \u00b6 Title \u00b6 Data Standardization: Improving Usability of Geospatial Data for Google Earth Engine Short Abstract \u00b6 Many geospatial datasets are difficult to access, navigate, and require preprocessing, creating a problem for data scientists performing planetary scale analyses. Data standardization creates a uniform structure, improving usability. The Spatio-Temporal Asset Catalog (STAC) specification designates the metadata for geospatial datasets. Using the principles of the STAC specification, we began standardizing Dr. Samapriya Roy\u2019s Awesome-Gee-Community-Datasets, a GitHub repository of 850+ Google Earth Engine (GEE), a platform for planetary geospatial analysis, datasets. The standardized metadata provides data scientists knowledge on how to use and understand the Awesome-Gee-Community-Datasets for GEE analysis. Standardization promotes discovery and open science, ensuring equity within computing and helping scientists work to understand Earth\u2019s processes, building a more aware and conscientious society.","title":"Assignments"},{"location":"keysassignments/#keys-assignments","text":"","title":"Keys Assignments"},{"location":"keysassignments/#assignment-1-internship-description","text":"This summer I am working in Dr. Tyson L. Swetnam's Lab. He is a Research Assistant Professor of Geoinformatics with a joint appointment in the School of Natural Resources and the Environment. Dr. Swetnam is part of the CyVerse initiative: a National Science Foundation funded cyberinfrastructure that promotes open source data, science, and collaboration in the biosciences. Dr. Swetnam's research is focused on using cyberinfrastructure to support and encourage reproducible research, along with geospatial analysis. The Lab's current projects include Open Dendro , an initiative to restructure outdated dendrochronological software written in C, R, and Fortran into Python. Dr. Swetnam is involved in a plethora of other projects, but all of his projects show a commitment to open science through the use of open data and open software to promote reproducible research in information science. The primary project that I will be focused on this summer is the improvement of spatio-temporal asset catalog (STAC) catalogs for use in Google's Earth Engine. Each spatio-temporal asset is a file or dataset that contains information about the Earth captured at a specific time. By streamlining this process and working on making these datasets more accessible it ensures equal access and promotes open science.","title":"Assignment 1: Internship Description"},{"location":"keysassignments/#project-description","text":"This summer I will be working on improving the STAC catalogs for the awesome-gee-community-datasets. This is a project spearheaded by Dr. Swetnam's former graduate student and colleague, Samapriya Roy. The project is focused on improving datasets for community use with Google's Earth Engine Catalog, a tool for mapping satellite and geospatial data across the Earth's surface to detect changes and trends. The awesome-gee-community-datasets are a set of community gathered and organized geospatial data that is preprocessed to allow for easy use with Google's Earth Engine. This summer I will work on improving the spatial-temporal asset (STAC) catalogs for these datasets to improve ease of use and access. A STAC catalog serves as a standardized way of storing and indexing geospatial data for easy discovery and use. This creates a common language and format that ensures previous code doesn't need to be rewritten, promoting open data, software, and science.","title":"Project Description"},{"location":"keysassignments/#assignment-2-introduction-to-your-research","text":"","title":"Assignment 2: Introduction to your Research"},{"location":"keysassignments/#purpose","text":"The purpose of my research is to standardize and process geospatial data for use with Google\u2019s Earth Engine for planetary-scale geospatial analysis. This improvement and standardization of geospatial data will be conducted using spatio-temporal asset catalogs (STAC), which will help make these complex datasets easier to index and discover in order to perform analyses of Earth\u2019s systems and conditions.","title":"Purpose:"},{"location":"keysassignments/#previous-research","text":"Oftentimes raw data is not ready for analysis, and it requires pre-processing. Additionally, this analysis often requires the installation and utilization of many different software packages. This is especially true for large datasets that are being used for planetary analysis on an Earth and solar system scale. However, the use of the spatio-temporal asset catalog (STAC) specification allows for the creation of metadata that allows datasets to be indexable and discoverable. A spatio-temporal asset is a type of Javascript object notation (JSON) file. This is a data interchange format that allows large raw datasets to be text based. The STAC specification designates the need for metadata on the time and location of the data, a thumbnail, links to the raw data, and key words that describe the relationship of the data, pointing to similar spatio-temporal assets. Following this specification furthers cloud based computing and means that these large geospatial datasets don\u2019t need to be downloaded locally for planetary scale analysis. It is for these reasons that previous research has shown that the STAC specification is an important and highly useful method for designating metadata, especially for large planetary scale analysis (Fergason et al, 2018). Additionally, this STAC specification can be used for Google\u2019s Earth Engine (GEE), established in 2010. GEE initially featured a data repository of global satellite data from the past 40 years; however, this has expanded to now include vector, climate, demographic, and elevation data that can be layered to perform complex global geospatial analyses. Users can also upload their own datasets and write scripts to analyze this data. The development of GEE as a free resource has helped to level the playing field and make large-scale geospatial analysis possible for a larger range of scientists, particularly those in developing countries (Kumar and Mutanga, 2018). However, a lot of this data that users are looking to use with GEE is not preproccessed, and STAC catalogs serve as a potential solution to this. My project will allow more equitable acces to a variety of geospatial data, promoting equality and open science. By improving the use of STAC catalogs it ensures that scientists have access to quality analysis tools like GEE and datasets that are easy to navigate.","title":"Previous Research:"},{"location":"keysassignments/#need-for-study","text":"This study will further the use of STAC catalogs for geospatial data. Through this standardization, it will make Google\u2019s Earth Engine more accessible to other researchers and data scientists. Pre-processing geospatial data and formatting it using STAC catalogs makes this data easier to index and search, allowing for its discovery. This concept of open data is crucial in regard to the open science movement. The production of open software and data allows key geospatial analysis tools to reach a larger number of researchers and scientists. With the threat of climate change and the necessity for immediate action, it is crucial to understand Earth\u2019s systems and processes. Google\u2019s Earth Engine allows for large-scale geospatial analysis that can help scientists visualize and understand concepts such as forest fire management and sea level change. The improvement of STAC catalogs for Google\u2019s Earth Engine facilitates analysis and discovery that is crucial to understanding how to prevent climate change and preserve the natural environment.","title":"Need For Study:"},{"location":"keysassignments/#problem-statement","text":"Many current geospatial datasets are difficult to navigate, discover, and analyze. How can the organization of this data be improved to ensure ease of use, appropriate analysis, and equitable access?","title":"Problem Statement:"},{"location":"keysassignments/#references","text":"Fergason, R. L., Hunter, M. A., Laura, J. R., Hare, T. M., & U.S. Geological Survey. (2021). Analysis Ready Data Available Through the SpatioTemporal Asset Catalog (STAC) Specification: Investigating the Application to Planetary Data. 5 th Planetary Data and PSIDA 2021, 2549, 7023\u20137024. https://www.hou.usra.edu/meetings/planetdata2021/pdf/7023.pdf Kumar, L., & Mutanga, O. (2018). Google Earth Engine Applications Since Inception: Usage, Trends, and Potential. Remote Sensing, 10(10), 1509. https://doi.org/10.3390/rs10101509","title":"References"},{"location":"keysassignments/#assignment-3-materials-and-methods","text":"To best standardize and organize geospatial data for use with Google\u2019s Earth Engine I will use the Spatio-Temporal Asset Catalog (STAC) Specification. The STAC Specification is the leading standard for geospatial data because it allows for the creation of metadata (data that provides information on the datasets) that allows assets to be indexable and discoverable. By following this specification I will create metadata for the awesome-gee-community datasets to improve reproducibility and promote discovery. Throughout this process I will use free software such as GitHub, VSCode, and Jupyter Notebooks to improve the metadata to align with the STAC Specification: - GitHub is a platform for software development that promotes collaboration and is committed to open science - VSCode is a code editor that provides syntax and debugging support - Jupyter Notebooks is a web-based computing platform that allows for the seamless integration of a code editor, terminal, and markdown files for documentation This software will aid me in completing my project efficiently and effectively. Additionally, their free nature and public access aligns with my project\u2019s mission of furthering open and equitable science. The initial organization of the awesome-gee-community datasets on GitHub is represented by this diagram: Using the aforementioned software, I will use the coding languages of Python, Markdown, and Javascript to create and improve the awesome-gee-community datasets\u2019 metadata to align with the STAC Specification, allowing its use with Google\u2019s Earth Engine.","title":"Assignment 3: Materials and Methods"},{"location":"keysassignments/#assignment-4-results","text":"I\u2019m still in the middle of conducting my research; therefore, I don\u2019t have completed results yet. Additionally, my project is somewhat unconventional in nature, as I am processing and standardizing datasets. This means that I won\u2019t have much numerical, collected data to graph or represent in charts. Instead, I plan to include information on which datasets I processed, and what the result of processing them was. I\u2019m currently working with around 50 datasets; however, I\u2019m not sure if I will work through all of these datasets during the internship and there is no logistical way to represent all of them on one poster. However, all of the datasets will be processed and standardized with the same template, producing a similar result but with different data. Therefore, my plan is to document the results of implementing this process for one example dataset, and then also include information on all of the other datasets that I processed. I haven\u2019t begun this process yet because my PI is still working on finalizing the guidelines and template for the process; therefore, I\u2019m not exactly sure what this will look like. I plan to represent my results mostly through visuals. I will include a screenshot of the final, processed spatio-temporal asset, along with a screenshot of this processed data visualized with Google Earth Engine that will look something like this: Ivushkin, Konstantin, Harm Bartholomeus, Arnold K. Bregt, Alim Pulatov, Bas Kempen, and Luis De Sousa. \"Global mapping of soil salinity change.\" Remote sensing of environment 231 (2019): 111260. I will then explain that all of the geospatial datasets were processed in this similar manner to achieve usable, standardized results. Additionally, I will include information on all of the geospatial data that I processed, along with what category it falls into, and its citation. I will represent this through a concept map/flowchart that will look something like this: Fig 1. Visual representation for listing the names of the standardized data sets and their corresponding overall categories My project is all done through computing, and therefore there is a large volume of code that can\u2019t be realistically all shown on my poster. However, throughout my internship I have been documenting my progress and results on a website I built. I need to further discuss this with my PI, but I also may include a QR code to my website under my results section for viewers who are interested in exploring my project more fully. Website QR code for extra information on my project and process","title":"Assignment 4: Results"},{"location":"keysassignments/#assignment-5-long-abstract","text":"Many geospatial datasets are difficult to access, require preprocessing, or the use of specialized software. This creates a problem for data scientists who are researching the Earth and performing planetary scale analyses: many of these datasets are hard to discover and navigate. In order to combat this issue, data standardization is necessary. The Spatio-Temporal Asset Catalog (STAC) specification provides a solution to this by designating the creation of metadata for each set of geospatial data, improving indexability and discovery. This data can be visualized and analyzed through Google\u2019s Earth Engine (GEE), a cloud based computing platform that allows for powerful, planetary scale geospatial analysis. We employed this standardization technique for Dr. Samapriya Roy\u2019s Awesome-Gee-Community-Datasets, a GitHub repository of over 850 geospatial datasets that can be analyzed and understood through GEE. By narrowing in on a subset of 49 of these datasets, Dr. Roy created a template that designated the creation and standardization of metadata for each of these datasets. This designated the need for information including a description, source data structure, GEE code snippet, license, and citation. This final, standardized metadata was displayed on the community datasets GitHub website, providing data scientists further knowledge on how to use, cite, and discover this data for analysis with GEE. Standardizing this data promotes discovery and the principles of open science, ensuring equity within computing through open data and open code. This is crucial as scientists work collaboratively to understand the planet\u2019s systems and processes to better combat climate change.","title":"Assignment 5 Long Abstract"},{"location":"keysassignments/#assignment-6-conclusion-and-discussion","text":"Due to the complex nature of large geospatial datasets, they can be difficult to analyze, requiring large amounts of preprocessing or software, stunting collaboration and scientific progress. This process for the standardization of the Community Datasets\u2019 metadata was successful in creating a more uniform format. By editing the MarkDown files to ensure the documentation was uniform throughout, it improved the user interface for the Community Datasets repository. We were not able to work through all of the forty-nine dataset subset, which was largely due to the long period of background research. In order to understand the current organizational status of the repository, and thus assess how to best standardize it, a significant period of time was spent taking notes and data on the initial state of the repository. While time consuming, this was a crucial step to ensure a replicable and effective technique for standardization, even if it meant sacrificing the quantity of standardized datasets. Additionally, some template elements, particularly the source data structure, were difficult, or not possible at this time, to ascertain. This meant that some of the standardized datasets did not perfectly match the initial template. Although some of the datasets did not align perfectly, the integrity of the project was still preserved, as usability and organization was still improved. This project will be continued to extend to the other datasets in the repository, hopefully eventually working through all remaining datasets, and ensuring that new datasets meet the specification when added to the repository. By improving the access and structure of these datasets, their discoverability is also improved. This upholds the tenets of open science through the principles of open data and open code. This plays an important role in improving equity and collaboration in computing. By providing data scientists increased and improved tools for analysis and discovery, it helps further understanding of Earth\u2019s systems and processes, helping us build a more unbiased, conscientious, and aware society.","title":"Assignment 6 Conclusion and Discussion"},{"location":"keysassignments/#assignment-7-title-and-short-abstract","text":"","title":"Assignment 7 Title and Short Abstract"},{"location":"keysassignments/#title","text":"Data Standardization: Improving Usability of Geospatial Data for Google Earth Engine","title":"Title"},{"location":"keysassignments/#short-abstract","text":"Many geospatial datasets are difficult to access, navigate, and require preprocessing, creating a problem for data scientists performing planetary scale analyses. Data standardization creates a uniform structure, improving usability. The Spatio-Temporal Asset Catalog (STAC) specification designates the metadata for geospatial datasets. Using the principles of the STAC specification, we began standardizing Dr. Samapriya Roy\u2019s Awesome-Gee-Community-Datasets, a GitHub repository of 850+ Google Earth Engine (GEE), a platform for planetary geospatial analysis, datasets. The standardized metadata provides data scientists knowledge on how to use and understand the Awesome-Gee-Community-Datasets for GEE analysis. Standardization promotes discovery and open science, ensuring equity within computing and helping scientists work to understand Earth\u2019s processes, building a more aware and conscientious society.","title":"Short Abstract"},{"location":"logbook/","text":"Logbook \u00b6 Day 1 (6/8) : Met with Dr. Swetnam and discussed project options and overall goals for the summer. Discussed working on the OpenDendro initiative or instead working with Google Earth Engine . Talked about how to setup this website using github: cloning Shruti's (a previous KEYS intern) repository and updating the information to match my own. Day 2 (6/9) : attempted to clone Shruti's repository through github's importer. However, this got stuck loading for 24+ hours so my progress was kind of at a standstill. Day 3 (6/10) : Cloning through github's importer never successfully loaded. I attempted to download all of Shruti's files to my computer and then add those files to my own repo to make a clone of her repo. However, Dr. Swetnam pointed out that after doing this I was still missing some key files and that the file paths and structure was incorrect. Day 4 (6/11) : Deleted contents of my new repository and this time cloned Shruti's repo through the terminal using git. After some trial and error (I had a very low level of familliarity with git) I got it to work! Day 5 (6/12) : Now that I had a successfully cloned repository, I worked on editing the mkdocs.yml to get my own website up and running. After setting up the basics and the URL, I added more personal information to further personalize my website. Day 6 (6/13) : Researched and enrolled in GitHub Education and CodeSpaces (turns out CodeSpaces isn't available as part of the student developer pack). Continued tweaking and cleaning up my GitHub site (read more of the mkdocs documentation to better understand this). Began research into Google Earth Engine and STAC Catalogs to build a foundation for my project this summer. Day 7-8 (6/14-15) : Gained increased familliarity with Google Earth Engine and accessed Google Earth Engine Developer. Forked Samapriya's awesome gee community datasets on GitHub to begin working with the repo with the intention of sending a pull request at the end of the summer to send it back to the main. Day 9 (6/16) : Continued to work with Google Earth Engine and began working with Jupyter Notebooks to better understand STAC catalogs. Worked to understand the formatting of the awesome-gee-community datasets by looking at the json files. Continued taking notes on both STAC catalogs and Google Earth Engine (adding these notes to the site tomorrow). Day 10 (6/17) : Continued to take notes to better understand the awesome-gee-community datasets and STAC catalogs. Began to transfer notes and information over into the github.io site (still need to make further progress with this). Experimented more with Jupyter Notebooks (need to commit more time to better understanding this). Talked to Dr. Swetnam about plans for meeting with Samapriya next week to discuss his vision for the awesome-gee-community datasets. Day 11 (6/20) : Worked through KEYS assignment on the introduction, background, and purpose of my research. Read additional research papers on both STAC catalogs and Google Earth Engine. Worked to create graphics in MKdocs to represent the structure of the awesome-gee-community datasets. Day 12 (6/21) : Today I fleshed out the STAC catalogs and GEE pages on my github.io site. Additionally, it turned out that my mkdocs diagrams are not rendering properly on my github.io site. I spent a long time researching the mkdocs and mermaid documentation for this, along with trying to debug it, but with no luck. I created a github issue about it explaining it more fully, but I hope to get this sorted out tomorrow. Experimented with visualizing different datasets on GEE and sifting through the GEE public data STAC catalog. Realized I should be taking more original pictures for my webpage and poster, so I will now start doing this as I move throughout the process. High on my personal agenda for tomorrow is to format all of the sources I've used/collected in APA style and properly cite them. Day 13 (6/22) : Met with Dr. Swetnam and Dr. Samapriya Roy to discuss the overall plan for the rest of my internship. Worked on setting up my computer to write and edit markdown files locally. Installed brew package on git, and set up vscode as a code editor to then render things locally before pushing to github. gained access to Samapriya's datasets on Earth Engine and will begin editing the markdown files for these on github based on a standardized template. Eventually may begin editing the JSONs for these and/or creating a Jupyter Notebooks manual for them. Also set up a twice weekly meeting with Samapriya. Day 14 (6/23) : Today I worked on formatting and gathering my references in order to do in text citations. I also did further work with Jupyter Notebooks and gained more familliarity with the VSCode software. Day 15 (6/24) : Today I worked on investigating and getting a better feel for the community datasets on GEE. Additionally, I updated my webpage with information on Jupyter Notebooks (I still want to add photos to this). I realized my mermaid diagrams are rendering correctly when I look at the page locally; however, they are still incorrect when I look at the actual website (even when it is up to date). I tried to get them to render correctly but I had no luck. Day 16 + 17 (6/27-28) : While waiting for the Markdown templates from Samapriya I have been working on creating graphics to better understand my research and improve clarity (the finished ones are up on my website). Additionally, I have been working on compiling a shortlist of some of the community datasets and recording information on these to streamline the standardization of these. Day 18, 19, 20, 21 (6/29-7/5) : Finished the long process of looking through all of the community dataset code on GEE, and taking notes on what catalog information they contained. Wrote and developed my elevator speech for my final KEYS presentation. Additionally, started working on formatting my poster; however, I cannot settle on a design that I'm happy with. Worked on devising a plan for how to best showcase my results and worked on designing poster graphics using BioRender.","title":"Logbook"},{"location":"logbook/#logbook","text":"Day 1 (6/8) : Met with Dr. Swetnam and discussed project options and overall goals for the summer. Discussed working on the OpenDendro initiative or instead working with Google Earth Engine . Talked about how to setup this website using github: cloning Shruti's (a previous KEYS intern) repository and updating the information to match my own. Day 2 (6/9) : attempted to clone Shruti's repository through github's importer. However, this got stuck loading for 24+ hours so my progress was kind of at a standstill. Day 3 (6/10) : Cloning through github's importer never successfully loaded. I attempted to download all of Shruti's files to my computer and then add those files to my own repo to make a clone of her repo. However, Dr. Swetnam pointed out that after doing this I was still missing some key files and that the file paths and structure was incorrect. Day 4 (6/11) : Deleted contents of my new repository and this time cloned Shruti's repo through the terminal using git. After some trial and error (I had a very low level of familliarity with git) I got it to work! Day 5 (6/12) : Now that I had a successfully cloned repository, I worked on editing the mkdocs.yml to get my own website up and running. After setting up the basics and the URL, I added more personal information to further personalize my website. Day 6 (6/13) : Researched and enrolled in GitHub Education and CodeSpaces (turns out CodeSpaces isn't available as part of the student developer pack). Continued tweaking and cleaning up my GitHub site (read more of the mkdocs documentation to better understand this). Began research into Google Earth Engine and STAC Catalogs to build a foundation for my project this summer. Day 7-8 (6/14-15) : Gained increased familliarity with Google Earth Engine and accessed Google Earth Engine Developer. Forked Samapriya's awesome gee community datasets on GitHub to begin working with the repo with the intention of sending a pull request at the end of the summer to send it back to the main. Day 9 (6/16) : Continued to work with Google Earth Engine and began working with Jupyter Notebooks to better understand STAC catalogs. Worked to understand the formatting of the awesome-gee-community datasets by looking at the json files. Continued taking notes on both STAC catalogs and Google Earth Engine (adding these notes to the site tomorrow). Day 10 (6/17) : Continued to take notes to better understand the awesome-gee-community datasets and STAC catalogs. Began to transfer notes and information over into the github.io site (still need to make further progress with this). Experimented more with Jupyter Notebooks (need to commit more time to better understanding this). Talked to Dr. Swetnam about plans for meeting with Samapriya next week to discuss his vision for the awesome-gee-community datasets. Day 11 (6/20) : Worked through KEYS assignment on the introduction, background, and purpose of my research. Read additional research papers on both STAC catalogs and Google Earth Engine. Worked to create graphics in MKdocs to represent the structure of the awesome-gee-community datasets. Day 12 (6/21) : Today I fleshed out the STAC catalogs and GEE pages on my github.io site. Additionally, it turned out that my mkdocs diagrams are not rendering properly on my github.io site. I spent a long time researching the mkdocs and mermaid documentation for this, along with trying to debug it, but with no luck. I created a github issue about it explaining it more fully, but I hope to get this sorted out tomorrow. Experimented with visualizing different datasets on GEE and sifting through the GEE public data STAC catalog. Realized I should be taking more original pictures for my webpage and poster, so I will now start doing this as I move throughout the process. High on my personal agenda for tomorrow is to format all of the sources I've used/collected in APA style and properly cite them. Day 13 (6/22) : Met with Dr. Swetnam and Dr. Samapriya Roy to discuss the overall plan for the rest of my internship. Worked on setting up my computer to write and edit markdown files locally. Installed brew package on git, and set up vscode as a code editor to then render things locally before pushing to github. gained access to Samapriya's datasets on Earth Engine and will begin editing the markdown files for these on github based on a standardized template. Eventually may begin editing the JSONs for these and/or creating a Jupyter Notebooks manual for them. Also set up a twice weekly meeting with Samapriya. Day 14 (6/23) : Today I worked on formatting and gathering my references in order to do in text citations. I also did further work with Jupyter Notebooks and gained more familliarity with the VSCode software. Day 15 (6/24) : Today I worked on investigating and getting a better feel for the community datasets on GEE. Additionally, I updated my webpage with information on Jupyter Notebooks (I still want to add photos to this). I realized my mermaid diagrams are rendering correctly when I look at the page locally; however, they are still incorrect when I look at the actual website (even when it is up to date). I tried to get them to render correctly but I had no luck. Day 16 + 17 (6/27-28) : While waiting for the Markdown templates from Samapriya I have been working on creating graphics to better understand my research and improve clarity (the finished ones are up on my website). Additionally, I have been working on compiling a shortlist of some of the community datasets and recording information on these to streamline the standardization of these. Day 18, 19, 20, 21 (6/29-7/5) : Finished the long process of looking through all of the community dataset code on GEE, and taking notes on what catalog information they contained. Wrote and developed my elevator speech for my final KEYS presentation. Additionally, started working on formatting my poster; however, I cannot settle on a design that I'm happy with. Worked on devising a plan for how to best showcase my results and worked on designing poster graphics using BioRender.","title":"Logbook"},{"location":"poster/","text":"","title":"Poster"},{"location":"references/","text":"References \u00b6","title":"References"},{"location":"references/#references","text":"","title":"References"},{"location":"standardizationLog/","text":"Preliminary analysis and notes on standardization GitHub fork of Dr. Roy's awesome-gee-community-datasets Website rendering of fork of Dr. Roy's awesome-gee-community-datasets","title":"Community Dataset Standardization Log"}]}